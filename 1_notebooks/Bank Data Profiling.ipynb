{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9deee936-3887-4768-968c-f9cb88ccddfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING AND PROFILING YOUR CSV FILE\n",
      "================================================================================\n",
      "Loading data from: bank_cleaned.csv\n",
      "[ERROR] File 'bank_cleaned.csv' not found.\n",
      "Please make sure the file path is correct and the file exists.\n",
      "\n",
      "================================================================================\n",
      "FALLBACK: USING SAMPLE DATA FOR DEMONSTRATION\n",
      "================================================================================\n",
      "================================================================================\n",
      "DATA PROFILING REPORT: Sample Customer Dataset\n",
      "================================================================================\n",
      "Report Generated: 2025-08-02 19:59:44\n",
      "\n",
      "BASIC INFORMATION\n",
      "----------------------------------------\n",
      "Total Rows: 1,020\n",
      "Total Columns: 6\n",
      "Memory Usage: 0.15 MB\n",
      "Duplicate Rows: 20 (1.96%)\n",
      "\n",
      "DATA TYPES ANALYSIS\n",
      "----------------------------------------\n",
      "     column   dtype  non_null_count  null_count  null_percentage  unique_values  unique_percentage\n",
      "customer_id   int64            1020           0         0.000000           1000          98.039216\n",
      "        age   int64            1020           0         0.000000             66           6.470588\n",
      "     income float64             968          52         5.098039            950          93.137255\n",
      "   category  object            1020           0         0.000000              4           0.392157\n",
      "      score float64             990          30         2.941176            971          95.196078\n",
      "     status  object            1020           0         0.000000              3           0.294118\n",
      "\n",
      "MISSING VALUES ANALYSIS\n",
      "----------------------------------------\n",
      "column  missing_count  missing_percentage data_type\n",
      "income             52            5.098039   float64\n",
      " score             30            2.941176   float64\n",
      "\n",
      "MISSING VALUE PATTERNS\n",
      "------------------------------\n",
      "No columns have >50% missing values.\n",
      "\n",
      "STATISTICAL SUMMARY\n",
      "----------------------------------------\n",
      "DESCRIPTIVE STATISTICS\n",
      "       customer_id       age     income    score\n",
      "count     1020.000  1020.000    968.000  990.000\n",
      "mean       504.526    34.760  51261.453   75.044\n",
      "std        289.863    11.714  15010.628   15.245\n",
      "min          1.000    -3.000   5894.170   30.133\n",
      "25%        252.750    27.000  41269.549   64.239\n",
      "50%        506.500    35.000  51091.144   75.211\n",
      "75%        757.250    42.000  61094.476   85.330\n",
      "max       1000.000    81.000  97896.614  133.894\n",
      "\n",
      "ADDITIONAL STATISTICS\n",
      "     column  skewness  kurtosis     variance  coefficient_of_variation\n",
      "customer_id   -0.0149   -1.2095 8.402059e+04                    0.5745\n",
      "        age    0.1150    0.0471 1.372210e+02                    0.3370\n",
      "     income   -0.0707    0.0790 2.253190e+08                    0.2928\n",
      "      score    0.0458   -0.0670 2.324095e+02                    0.2031\n",
      "\n",
      "OUTLIER DETECTION\n",
      "----------------------------------------\n",
      "column  outlier_count  outlier_percentage  lower_bound  upper_bound  min_outlier  max_outlier\n",
      "   age             11               1.078        4.500       64.500       -3.000       81.000\n",
      "income              9               0.930    11532.160    90831.865     5894.170    97896.614\n",
      " score              7               0.707       32.603      116.966       30.133      133.894\n",
      "\n",
      "CORRELATION ANALYSIS\n",
      "----------------------------------------\n",
      "No high correlations (|r| > 0.7) found between numerical variables.\n",
      "\n",
      "CATEGORICAL VARIABLES ANALYSIS\n",
      "----------------------------------------\n",
      "  column  unique_values most_frequent_value  most_frequent_count  most_frequent_percentage least_frequent_value  least_frequent_count\n",
      "category              4                   A                  277                 27.156863                    B                   240\n",
      "  status              3              Active                  631                 61.862745              Pending                   101\n",
      "\n",
      "TOP 5 VALUES PER CATEGORICAL COLUMN\n",
      "--------------------------------------------------\n",
      "\n",
      "CATEGORY:\n",
      "  A: 277 (27.2%)\n",
      "  D: 258 (25.3%)\n",
      "  C: 245 (24.0%)\n",
      "  B: 240 (23.5%)\n",
      "\n",
      "STATUS:\n",
      "  Active: 631 (61.9%)\n",
      "  Inactive: 288 (28.2%)\n",
      "  Pending: 101 (9.9%)\n",
      "\n",
      "DATA QUALITY ASSESSMENT\n",
      "----------------------------------------\n",
      "Completeness Score: 98.66%\n",
      "Uniqueness Score: 98.04%\n",
      "Consistency Score: 100.00%\n",
      "Overall Quality Score: 98.90%\n",
      "\n",
      "QUALITY INTERPRETATION:\n",
      "[EXCELLENT] Excellent data quality\n",
      "\n",
      "RECOMMENDATIONS\n",
      "----------------------------------------\n",
      "No specific recommendations - your data quality appears to be good!\n",
      "\n",
      "================================================================================\n",
      "DATA PROFILING REPORT COMPLETED\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class DataProfiler:\n",
    "    \n",
    "    def __init__(self, dataframe, dataset_name=\"Dataset\"):\n",
    "       \n",
    "        self.df = dataframe.copy()\n",
    "        self.dataset_name = dataset_name\n",
    "        self.profile_results = {}\n",
    "        \n",
    "    def basic_info(self):\n",
    "       \n",
    "        print(\"=\"*80)\n",
    "        print(f\"DATA PROFILING REPORT: {self.dataset_name}\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Report Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print()\n",
    "        \n",
    "        basic_stats = {\n",
    "            'total_rows': len(self.df),\n",
    "            'total_columns': len(self.df.columns),\n",
    "            'memory_usage_mb': self.df.memory_usage(deep=True).sum() / 1024**2,\n",
    "            'duplicate_rows': self.df.duplicated().sum(),\n",
    "            'duplicate_percentage': (self.df.duplicated().sum() / len(self.df)) * 100\n",
    "        }\n",
    "        \n",
    "        print(\"BASIC INFORMATION\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Total Rows: {basic_stats['total_rows']:,}\")\n",
    "        print(f\"Total Columns: {basic_stats['total_columns']:,}\")\n",
    "        print(f\"Memory Usage: {basic_stats['memory_usage_mb']:.2f} MB\")\n",
    "        print(f\"Duplicate Rows: {basic_stats['duplicate_rows']:,} ({basic_stats['duplicate_percentage']:.2f}%)\")\n",
    "        print()\n",
    "        \n",
    "        self.profile_results['basic_info'] = basic_stats\n",
    "        return basic_stats\n",
    "    \n",
    "    def data_types_analysis(self):\n",
    "       \n",
    "        print(\"DATA TYPES ANALYSIS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        dtype_info = []\n",
    "        for col in self.df.columns:\n",
    "            col_info = {\n",
    "                'column': col,\n",
    "                'dtype': str(self.df[col].dtype),\n",
    "                'non_null_count': self.df[col].count(),\n",
    "                'null_count': self.df[col].isnull().sum(),\n",
    "                'null_percentage': (self.df[col].isnull().sum() / len(self.df)) * 100,\n",
    "                'unique_values': self.df[col].nunique(),\n",
    "                'unique_percentage': (self.df[col].nunique() / len(self.df)) * 100\n",
    "            }\n",
    "            dtype_info.append(col_info)\n",
    "        \n",
    "        dtype_df = pd.DataFrame(dtype_info)\n",
    "        print(dtype_df.to_string(index=False))\n",
    "        print()\n",
    "        \n",
    "        self.profile_results['data_types'] = dtype_df\n",
    "        return dtype_df\n",
    "    \n",
    "    def missing_values_analysis(self):\n",
    "       \n",
    "        print(\"MISSING VALUES ANALYSIS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        missing_info = []\n",
    "        for col in self.df.columns:\n",
    "            missing_count = self.df[col].isnull().sum()\n",
    "            missing_percentage = (missing_count / len(self.df)) * 100\n",
    "            \n",
    "            if missing_count > 0:\n",
    "                missing_info.append({\n",
    "                    'column': col,\n",
    "                    'missing_count': missing_count,\n",
    "                    'missing_percentage': missing_percentage,\n",
    "                    'data_type': str(self.df[col].dtype)\n",
    "                })\n",
    "        \n",
    "        if missing_info:\n",
    "            missing_df = pd.DataFrame(missing_info)\n",
    "            missing_df = missing_df.sort_values('missing_percentage', ascending=False)\n",
    "            print(missing_df.to_string(index=False))\n",
    "            \n",
    "            print(\"\\nMISSING VALUE PATTERNS\")\n",
    "            print(\"-\" * 30)\n",
    "            high_missing = missing_df[missing_df['missing_percentage'] > 50]\n",
    "            if not high_missing.empty:\n",
    "                print(\"Columns with >50% missing values:\")\n",
    "                for _, row in high_missing.iterrows():\n",
    "                    print(f\"• {row['column']}: {row['missing_percentage']:.1f}% missing\")\n",
    "            else:\n",
    "                print(\"No columns have >50% missing values.\")\n",
    "        else:\n",
    "            print(\"No missing values found in the dataset.\")\n",
    "        \n",
    "        print()\n",
    "        self.profile_results['missing_values'] = missing_info\n",
    "        return missing_info\n",
    "    \n",
    "    def statistical_summary(self):\n",
    "       \n",
    "        print(\"STATISTICAL SUMMARY\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        numerical_cols = self.df.select_dtypes(include=[np.number]).columns\n",
    "        \n",
    "        if len(numerical_cols) > 0:\n",
    "            stats_summary = self.df[numerical_cols].describe()\n",
    "            print(\"DESCRIPTIVE STATISTICS\")\n",
    "            print(stats_summary.round(3))\n",
    "            print()\n",
    "            \n",
    "            additional_stats = []\n",
    "            for col in numerical_cols:\n",
    "                col_data = self.df[col].dropna()\n",
    "                if len(col_data) > 0:\n",
    "                    stats_info = {\n",
    "                        'column': col,\n",
    "                        'skewness': stats.skew(col_data),\n",
    "                        'kurtosis': stats.kurtosis(col_data),\n",
    "                        'variance': col_data.var(),\n",
    "                        'coefficient_of_variation': col_data.std() / col_data.mean() if col_data.mean() != 0 else np.inf\n",
    "                    }\n",
    "                    additional_stats.append(stats_info)\n",
    "            \n",
    "            if additional_stats:\n",
    "                additional_df = pd.DataFrame(additional_stats)\n",
    "                print(\"ADDITIONAL STATISTICS\")\n",
    "                print(additional_df.round(4).to_string(index=False))\n",
    "            \n",
    "            self.profile_results['statistical_summary'] = {\n",
    "                'descriptive': stats_summary,\n",
    "                'additional': additional_df if additional_stats else None\n",
    "            }\n",
    "        else:\n",
    "            print(\"No numerical columns found for statistical analysis.\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    def outlier_detection(self):\n",
    "      \n",
    "        print(\"OUTLIER DETECTION\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        numerical_cols = self.df.select_dtypes(include=[np.number]).columns\n",
    "        outlier_info = []\n",
    "        \n",
    "        for col in numerical_cols:\n",
    "            col_data = self.df[col].dropna()\n",
    "            if len(col_data) > 0:\n",
    "                Q1 = col_data.quantile(0.25)\n",
    "                Q3 = col_data.quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                \n",
    "                outliers = col_data[(col_data < lower_bound) | (col_data > upper_bound)]\n",
    "                outlier_count = len(outliers)\n",
    "                outlier_percentage = (outlier_count / len(col_data)) * 100\n",
    "                \n",
    "                if outlier_count > 0:\n",
    "                    outlier_info.append({\n",
    "                        'column': col,\n",
    "                        'outlier_count': outlier_count,\n",
    "                        'outlier_percentage': outlier_percentage,\n",
    "                        'lower_bound': lower_bound,\n",
    "                        'upper_bound': upper_bound,\n",
    "                        'min_outlier': outliers.min(),\n",
    "                        'max_outlier': outliers.max()\n",
    "                    })\n",
    "        \n",
    "        if outlier_info:\n",
    "            outlier_df = pd.DataFrame(outlier_info)\n",
    "            print(outlier_df.round(3).to_string(index=False))\n",
    "        else:\n",
    "            print(\"No outliers detected using IQR method.\")\n",
    "        \n",
    "        print()\n",
    "        self.profile_results['outliers'] = outlier_info\n",
    "        return outlier_info\n",
    "    \n",
    "    def correlation_analysis(self):\n",
    "       \n",
    "        print(\"CORRELATION ANALYSIS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        numerical_cols = self.df.select_dtypes(include=[np.number]).columns\n",
    "        \n",
    "        if len(numerical_cols) > 1:\n",
    "            correlation_matrix = self.df[numerical_cols].corr()\n",
    "            \n",
    "            high_corr_pairs = []\n",
    "            for i in range(len(correlation_matrix.columns)):\n",
    "                for j in range(i+1, len(correlation_matrix.columns)):\n",
    "                    corr_value = correlation_matrix.iloc[i, j]\n",
    "                    if abs(corr_value) > 0.7:  # High correlation threshold\n",
    "                        high_corr_pairs.append({\n",
    "                            'variable_1': correlation_matrix.columns[i],\n",
    "                            'variable_2': correlation_matrix.columns[j],\n",
    "                            'correlation': corr_value\n",
    "                        })\n",
    "            \n",
    "            if high_corr_pairs:\n",
    "                print(\"HIGH CORRELATIONS (|r| > 0.7):\")\n",
    "                high_corr_df = pd.DataFrame(high_corr_pairs)\n",
    "                high_corr_df = high_corr_df.sort_values('correlation', key=abs, ascending=False)\n",
    "                print(high_corr_df.round(4).to_string(index=False))\n",
    "            else:\n",
    "                print(\"No high correlations (|r| > 0.7) found between numerical variables.\")\n",
    "            \n",
    "            self.profile_results['correlation'] = {\n",
    "                'matrix': correlation_matrix,\n",
    "                'high_correlations': high_corr_pairs\n",
    "            }\n",
    "        else:\n",
    "            print(\"Insufficient numerical columns for correlation analysis.\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    def categorical_analysis(self):\n",
    "       \n",
    "        print(\"CATEGORICAL VARIABLES ANALYSIS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        categorical_cols = self.df.select_dtypes(include=['object', 'category']).columns\n",
    "        \n",
    "        if len(categorical_cols) > 0:\n",
    "            cat_info = []\n",
    "            for col in categorical_cols:\n",
    "                col_data = self.df[col].dropna()\n",
    "                value_counts = col_data.value_counts()\n",
    "                \n",
    "                cat_stats = {\n",
    "                    'column': col,\n",
    "                    'unique_values': len(value_counts),\n",
    "                    'most_frequent_value': value_counts.index[0] if len(value_counts) > 0 else None,\n",
    "                    'most_frequent_count': value_counts.iloc[0] if len(value_counts) > 0 else 0,\n",
    "                    'most_frequent_percentage': (value_counts.iloc[0] / len(col_data)) * 100 if len(value_counts) > 0 else 0,\n",
    "                    'least_frequent_value': value_counts.index[-1] if len(value_counts) > 0 else None,\n",
    "                    'least_frequent_count': value_counts.iloc[-1] if len(value_counts) > 0 else 0\n",
    "                }\n",
    "                cat_info.append(cat_stats)\n",
    "            \n",
    "            cat_df = pd.DataFrame(cat_info)\n",
    "            print(cat_df.to_string(index=False))\n",
    "            \n",
    "            print(\"\\nTOP 5 VALUES PER CATEGORICAL COLUMN\")\n",
    "            print(\"-\" * 50)\n",
    "            for col in categorical_cols:\n",
    "                print(f\"\\n{col.upper()}:\")\n",
    "                top_values = self.df[col].value_counts().head()\n",
    "                for value, count in top_values.items():\n",
    "                    percentage = (count / len(self.df)) * 100\n",
    "                    print(f\"  {value}: {count} ({percentage:.1f}%)\")\n",
    "            \n",
    "            self.profile_results['categorical'] = cat_df\n",
    "        else:\n",
    "            print(\"No categorical columns found in the dataset.\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    def data_quality_assessment(self):\n",
    "      \n",
    "        print(\"DATA QUALITY ASSESSMENT\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        quality_scores = {}\n",
    "        \n",
    "        total_cells = len(self.df) * len(self.df.columns)\n",
    "        missing_cells = self.df.isnull().sum().sum()\n",
    "        completeness_score = ((total_cells - missing_cells) / total_cells) * 100\n",
    "        quality_scores['completeness'] = completeness_score\n",
    "        \n",
    "        uniqueness_score = ((len(self.df) - self.df.duplicated().sum()) / len(self.df)) * 100\n",
    "        quality_scores['uniqueness'] = uniqueness_score\n",
    "        \n",
    "        consistency_issues = 0\n",
    "        for col in self.df.columns:\n",
    "            if self.df[col].dtype == 'object':\n",
    "            \n",
    "                non_null_values = self.df[col].dropna()\n",
    "                if len(non_null_values) > 0:\n",
    "                 \n",
    "                    numeric_count = sum(str(val).replace('.', '').replace('-', '').isdigit() for val in non_null_values)\n",
    "                    if 0 < numeric_count < len(non_null_values):\n",
    "                        consistency_issues += 1\n",
    "        \n",
    "        consistency_score = ((len(self.df.columns) - consistency_issues) / len(self.df.columns)) * 100\n",
    "        quality_scores['consistency'] = consistency_score\n",
    "        \n",
    "        overall_score = (completeness_score + uniqueness_score + consistency_score) / 3\n",
    "        quality_scores['overall'] = overall_score\n",
    "        \n",
    "        print(f\"Completeness Score: {completeness_score:.2f}%\")\n",
    "        print(f\"Uniqueness Score: {uniqueness_score:.2f}%\")\n",
    "        print(f\"Consistency Score: {consistency_score:.2f}%\")\n",
    "        print(f\"Overall Quality Score: {overall_score:.2f}%\")\n",
    "        \n",
    "        print(\"\\nQUALITY INTERPRETATION:\")\n",
    "        if overall_score >= 90:\n",
    "            print(\"[EXCELLENT] Excellent data quality\")\n",
    "        elif overall_score >= 80:\n",
    "            print(\"[GOOD] Good data quality\")\n",
    "        elif overall_score >= 70:\n",
    "            print(\"[WARNING] Fair data quality - some improvements needed\")\n",
    "        else:\n",
    "            print(\"[WARNING] Poor data quality - significant improvements required\")\n",
    "        \n",
    "        print()\n",
    "        self.profile_results['quality_scores'] = quality_scores\n",
    "        return quality_scores\n",
    "    \n",
    "    def generate_recommendations(self):\n",
    "       \n",
    "        print(\"RECOMMENDATIONS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        recommendations = []\n",
    "        \n",
    "        if 'missing_values' in self.profile_results and self.profile_results['missing_values']:\n",
    "            high_missing = [item for item in self.profile_results['missing_values'] \n",
    "                          if item['missing_percentage'] > 50]\n",
    "            if high_missing:\n",
    "                recommendations.append(\"Consider removing columns with >50% missing values or investigate data collection issues\")\n",
    "        \n",
    "        if 'basic_info' in self.profile_results:\n",
    "            if self.profile_results['basic_info']['duplicate_percentage'] > 5:\n",
    "                recommendations.append(\"High percentage of duplicate rows detected - consider data deduplication\")\n",
    "        \n",
    "        if 'outliers' in self.profile_results and self.profile_results['outliers']:\n",
    "            high_outlier_cols = [item for item in self.profile_results['outliers'] \n",
    "                               if item['outlier_percentage'] > 10]\n",
    "            if high_outlier_cols:\n",
    "                recommendations.append(\"Several columns have high outlier percentages - investigate data collection or consider outlier treatment\")\n",
    "        \n",
    "        if 'correlation' in self.profile_results and self.profile_results['correlation']['high_correlations']:\n",
    "            recommendations.append(\"High correlations detected between variables - consider feature selection or dimensionality reduction\")\n",
    "        \n",
    "        if 'quality_scores' in self.profile_results:\n",
    "            if self.profile_results['quality_scores']['overall'] < 80:\n",
    "                recommendations.append(\"Overall data quality is below 80% - prioritize data cleaning and validation\")\n",
    "        \n",
    "        if recommendations:\n",
    "            for i, rec in enumerate(recommendations, 1):\n",
    "                print(f\"{i}. {rec}\")\n",
    "        else:\n",
    "            print(\"No specific recommendations - your data quality appears to be good!\")\n",
    "        \n",
    "        print()\n",
    "        self.profile_results['recommendations'] = recommendations\n",
    "        return recommendations\n",
    "    \n",
    "    def generate_full_report(self):\n",
    "        \n",
    "        self.basic_info()\n",
    "        self.data_types_analysis()\n",
    "        self.missing_values_analysis()\n",
    "        self.statistical_summary()\n",
    "        self.outlier_detection()\n",
    "        self.correlation_analysis()\n",
    "        self.categorical_analysis()\n",
    "        self.data_quality_assessment()\n",
    "        self.generate_recommendations()\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        print(\"DATA PROFILING REPORT COMPLETED\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        return self.profile_results\n",
    "\n",
    "\n",
    "def profile_dataset(df, dataset_name=\"Dataset\"):\n",
    "    \n",
    "    profiler = DataProfiler(df, dataset_name)\n",
    "    return profiler.generate_full_report()\n",
    "\n",
    "\n",
    "def load_and_profile_csv(file_path, dataset_name=None):\n",
    "   \n",
    "    try:\n",
    "        print(f\"Loading data from: {file_path}\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"[SUCCESS] Successfully loaded {len(df)} rows and {len(df.columns)} columns\")\n",
    "        print()\n",
    "        \n",
    "        if dataset_name is None:\n",
    "            dataset_name = file_path.split('/')[-1].split('\\\\')[-1].replace('.csv', '').replace('_', ' ').title()\n",
    "        \n",
    "        results = profile_dataset(df, dataset_name)\n",
    "        \n",
    "        return df, results\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"[ERROR] File '{file_path}' not found.\")\n",
    "        print(\"Please make sure the file path is correct and the file exists.\")\n",
    "        return None, None\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(\"[ERROR] The CSV file is empty.\")\n",
    "        return None, None\n",
    "    except pd.errors.ParserError as e:\n",
    "        print(f\"[ERROR] Error parsing CSV file: {e}\")\n",
    "        print(\"Please check if the file is properly formatted.\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Unexpected error: {e}\")\n",
    "        return None, None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    csv_file_path = \"bank_cleaned.csv\" \n",
    "    \n",
    "    print(\"LOADING AND PROFILING YOUR CSV FILE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    df, results = load_and_profile_csv(csv_file_path)\n",
    "    \n",
    "    if df is not None:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ADDITIONAL ANALYSIS OPTIONS\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"Your data has been loaded successfully!\")\n",
    "        print(f\"Dataset shape: {df.shape}\")\n",
    "        print(\"\\nFirst few rows of your data:\")\n",
    "        print(df.head())\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"FALLBACK: USING SAMPLE DATA FOR DEMONSTRATION\")\n",
    "        print(\"=\"*80)\n",
    "       \n",
    "        np.random.seed(42)\n",
    "        sample_data = {\n",
    "            'customer_id': range(1, 1001),\n",
    "            'age': np.random.normal(35, 12, 1000).astype(int),\n",
    "            'income': np.random.normal(50000, 15000, 1000),\n",
    "            'category': np.random.choice(['A', 'B', 'C', 'D'], 1000),\n",
    "            'score': np.random.normal(75, 15, 1000),\n",
    "            'status': np.random.choice(['Active', 'Inactive', 'Pending'], 1000, p=[0.6, 0.3, 0.1])\n",
    "        }\n",
    "\n",
    "        sample_df = pd.DataFrame(sample_data)\n",
    "        \n",
    "        sample_df.loc[np.random.choice(sample_df.index, 50), 'income'] = np.nan\n",
    "        sample_df.loc[np.random.choice(sample_df.index, 30), 'score'] = np.nan\n",
    "        \n",
    "        duplicate_rows = sample_df.sample(20)\n",
    "        sample_df = pd.concat([sample_df, duplicate_rows], ignore_index=True)\n",
    "        \n",
    "        results = profile_dataset(sample_df, \"Sample Customer Dataset\")\n",
    "\n",
    "def quick_preview(file_path, rows=5):\n",
    "   \n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Dataset Preview: {file_path}\")\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        print(\"\\nFirst {rows} rows:\")\n",
    "        print(df.head(rows))\n",
    "        print(\"\\nColumn names:\")\n",
    "        print(df.columns.tolist())\n",
    "        print(\"\\nData types:\")\n",
    "        print(df.dtypes)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file: {e}\")\n",
    "        return None\n",
    "\n",
    "def save_profiling_report(results, output_file=\"profiling_report.txt\"):\n",
    "    \n",
    "    try:\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(\"DATA PROFILING REPORT\\n\")\n",
    "            f.write(\"=\"*50 + \"\\n\")\n",
    "            f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "            \n",
    "            if 'basic_info' in results:\n",
    "                f.write(\"BASIC INFORMATION\\n\")\n",
    "                f.write(\"-\"*30 + \"\\n\")\n",
    "                for key, value in results['basic_info'].items():\n",
    "                    f.write(f\"{key}: {value}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            if 'recommendations' in results:\n",
    "                f.write(\"RECOMMENDATIONS\\n\")\n",
    "                f.write(\"-\"*30 + \"\\n\")\n",
    "                for i, rec in enumerate(results['recommendations'], 1):\n",
    "                    f.write(f\"{i}. {rec}\\n\")\n",
    "        \n",
    "        print(f\"[SUCCESS] Profiling report saved to: {output_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving report: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d9ae38-603c-48b5-8c4f-0db40da00da0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
